# G検定メモ

## 第１章 AIとは

AIは**計算機による知的な情報処理システムの設計や実現**に関する研究分野。

### 1-1. 人工知能の分類

1. シンプルな制御プログラム  
   すべての振る舞いが予め決定されている。
   > e.g. 洗濯機の水量調整、エアコンの温度調整
2. 古典的な人工知能  
   探索・推論、知識データを利用して状況に応じた振る舞いをする
   > e.g. 掃除ロボット、診断プログラム
3. 機械学習を取り入れた人工知能  
   膨大なサンプルデータを下に入力と出力の関係を学習する
   > e.g. 検索エンジン、交通渋滞予測
4. Deep Learningを取り入れた人工知能  
   得寮寮変数を自動的に学習する
   - 特徴量：特徴が学習結果に大きく影響するかを判定している

### 1-2. 1人工知能研究の歴史

- 第一次AIブーム（推論・探索の時代）  
  - 1946年：汎用コンピュータENIACの完成
  - 1956年：ダートマス会議で「人工知能」の登場
    - マーヴィン・ミンスキー
    - ジョン・マッカーシー  
      > 人工知能という言葉を使い始めた
    - アレン・ニューウェル
    - ハーバード・サイモン
  - 1960年：解決できるの**トイ・プロブレム**だけかよ、がっかりだぜ
- 第二次AIブーム（知識の時代）  
  - 1980年：DBを使った**エキスパート・システム**の完成
    > 日本では**第５世代コンピュータ**とかダッセェ名前がつけられる
  - 1995年：知識膨大すぎて人間には管理できねーじゃん、バカヤロー
  - **フレーム問題**解けないじゃん
  - 第三次AIブーム（機械学習・特徴表現学習の時代）
    **ビッグデータ**とかいうバズワードを使って機械学習　　
    特徴量という知識要素を機械に学習させるDeep Learningが話題に  
    > レイ・カーツワイル「**シンギュラリティー**は2045年に到来する」

## 第２章 人工知能をめぐる動向

### 2-1. 探索・推論

迷路問題は**探索木**に置き換えているだけ。

**場合分け**を解くには探索法を用いる。

- 幅優先探索
- 深さ優先探索

#### 幅優先探索

階層を最優先要素として、その階層を探索したら次の階層を探索する方法。  
階層は下方向なので深さ優先と思いがちだが、その階層を全部探索しないと次に行けないので横方向が最優先要素であると理解すればいい。

階層を全チェックすることから、常に最短ゴールすることができる。  
ただし、ルートを全記憶する必要があるのでメモリ不足に悩まされる。

#### 深さ優先探索

とりあえず、末節まで行くまで行ってそれが答えか判断する方法。

答えじゃなかったらそのルートをメモリから削除できるのでメモリ不足は発生しにくい。  
ただ、それが答えかどうかは運ゲーなので時間がかかる。

### ロボットの行動計画

ロボットも結局は探索。  
**プランニング**と呼ぶ。

- STRIPS(Stanford Research Institute Problem Solver)  
  前提条件・行動・結果の組み合わせを
- SHRDLU（しゅるどぅるー）  
  テリー・ウィノグラードのプログラミング言語。システムか。  
  **Cyc**プロジェクトに派生する
  テリー・ウィノグラードはGoogleのラリー・ペイジを育てている。

### ボードゲーム

Google傘下に下ったディープマインド社の**AlphaGo**  
もはや、探索法では無理ゲー

|ゲーム|目地数|組み合わせ数|
|:--:|:--:|:--|
|オセロ|8x8|白黒裏返しあり→約10^60|
|チェス|8x8|白黒6種→約10^120|
|将棋|9x9|8種。ただし、獲得駒が使える→約10^220|
|囲碁|19x19|白黒→約10^360|

#### Min-Max法

自分が指すときにスコアが最大になるようにする。

- アルファカット  
  最小スコアを選ぶ過程で、すでに出現したスコアより大きいノードが出たらそれ以降をスキップする
- ベータカット  
  最大スコアを選ぶ過程で、スコアが小さいノードが出たらそれ以降をスキップする

#### モンテカルロ法

ゲームは探索数が膨大なだけでなく、その**コスト評価**に原因があるという考え。  
ある程度ゲームが進むと、スコア評価を放棄する。

コンピューターは秒間ですごい数のシミュレーションができるので、２プレイヤー用意して無限に手を打たせ続ける訳ですよ。序盤は盤面が広すぎてシミュレーションパターン多すぎて無理ですけど、後半盤面が小さくなってくるとパターン洗い出した方が良い成果が出るって寸法。
簡単に言うとブルートフォースシミュレーションみたいな。

## 2-2. 知識表現

### 人工無能

チャットボットとか。

- ELIZA（イライザ）  
  1964年頃からジョゼフ・ワイゼンバウムによって作成された。  
  相手の発言を予め用意されたパターンと比較して合致したパターンを返す。  
  相手が会話していると勘違いすることを**イライザ効果**と言う。
  > うーん、シーマンみたいな？  
  > アレはシーマン側から質問するからAIじゃないけど

- PARRY


### エキスパートシステム

- MYCIN（マイシン）  
  バクテリアの診断支援用エキスパートシステム
- DENDRAL  
  1960年に未知の有機化合物を特定するエキスパートシステム

#### エキスパートシステムの限界

エキスパートシステムは知識ベースが必要だが、人間の知識は経験則が多く暗黙的である。  
インタビューシステムも作られたが、一貫していない矛盾した知識もあり、知識ベースの保守が無理ゲーだった。

### 意味ネットワーク(semantic network)

認知心理学における長期記憶の構造モデルとして考案された。
is-a関係とかpart-of関係とかで色々書く。

- セマンティック・ウェブ  
  情報リソースに意味を付与することで、コンピュータで高度な意味処理を実現する。
- 意味ネットワーク  
  単語同士の意味関係をネットワークで表現する。
- 統計的自然言語処理  
  言語処理に確率論あるいは統計学的手法を用いる。  
  ここ20年で最も発達した。

### Cycプロジェクト

エキスパートシステムは知識ベースをうまく作用させるには、広範囲に及ぶ常識が必要になる。  
つまり、**一般常識をコンピュータに取り込もうプロジェクト**
ダグラス・レナートが1984年からスターと

#### オントロジー

知識の記述・整理が無理ゲーだったので**知識を体系化する手法**。
知識の共有と活用を目的とする。

- 重量オントロジー（ヘビーウェイトオントロジー）  
  意味論を人間が考えて手入力する方法。  
  Cycプロジェクトがソレ。
- 軽量オントロジー（ライトウェイトオントロジー）  
  自動解析する**ウェブマイニング**とか**データマイニング**とか  
  本当に正確じゃなくても使えるレベルに正確だったら良いよね論。
  BIMのワトソンとか。

#### ワトソン

IBMのAI。  
次の順番で処理。

1. 質問を分析
2. 複数の回答を候補
3. 質問との整合性・条件チェック
4. 総合点数の最高点を回答として採用

**質問の意味を理解するのではなく、質問に関連するキーワードを高速検索している。**

## 2-3. 機械学習

ビッグデータを使ってる

- レコメンデーションエンジン
- スパムフィルター

### ニューラルネットワーク

1958年：単純パーセプトロン

### ディープラーニング

1960年頃に考案され、2010年代に脚光を浴びたアルゴリズム。
ニューラルネットワークを多層にしたもの。

ILSVRCは画像認識コンペティション

## 第3章 人工知能分野の問題

### フレーム問題

1969年、ジョン・マッカーシー  
「今しようとしておることに関係がある事柄だけを選び出すことが、実は非常に難しい」

1. そのまま持ち出して爆発
2. 持ち出す方法の計算中に爆発
3. 持ち出す方法の計算方法を計算中に爆発

### チューリングテスト

アラン・チューリング
「別の場所にいる人間がコンピュータと会話し、相手がコンピュータだと見抜けなければコンピュータには知能があるとする」

AIが知的かどうかを評価するテスト。  
何も知らない審査員を騙し通せたら知性ありとして認定。

### 強いAIと弱いAI

- 強いAI  
  適切にプログラムされたコンピュータは人間が心を持つのと同じ意味で心を持つ
- 弱いAI  
  コンピュータは人間の心を持つ必要はなく、有用な道具であればいい

ジョン・サール：中国語の部屋

### シンボルグラウンディング問題

スティーブン・ハルナッド  
「シンボルとその対象が以下にして結びつくか」

シマウマは縞と馬だが、コンピュータにその意味はわからない

### 知識獲得のボトルネック

- 1970年：ルールベース機械翻訳
- 1990年：統計的機械翻訳

## 第４章 機械学習の具体的手法

- 教師あり学習
- 教師なし学習
- 強化学習

### 教師あり学習

- 回帰  
  入力データ対応する実数の出力値を近似する。
  - 線形回帰  
    - ラッソ回帰（L1正則化）
    - リッジ回帰（L2正則化）
- 分類  
  入力がどのクラスに属するか
  - サポートベクターマシン
  - 決定木
  - ランダムフォレスト
  - kNN法
  - ロジスティック回帰

#### ロジスティック回帰

- ２値分類：シグモイド関数
- 多値分類：ソフトマックス関数

0.5を境にして正負を決めるなどすることができる。  
しかし、スパムフィルターの感度を上げるために閾値0.5を上げて0.7にするとかいうケースも有る。

基本的な流れ
1. データがクラスiに属する確率の**対数オッズ**を用いて線形回帰予測する
2. **対数オッズ**に**ロジット変換**を施してクラスiに属する確率の予測値を得る。
3. 全クラスの確率を算出して、最大値を持つものを予測データとする

#### ランダムフォレスト

教師あり学習は、入力の特徴量をパターン出力するに尽きる。  
そこで、特徴量の値を順々に木構造にしていけば、最終的には答えに辿り着くだろうというモノ。  
木構造はランダムに作り、それらを並行して複数実行することで、最終的には多数決で答えを出す。多数決を行うことで集合知になり、モデルの精度が極端に悪い奴がいても均されていい感じの結果になる、

**ブートストラップサンプリング**を用いる。  
> データ全てを使わずに一部分だけランダムに使用するサンプリング方法

- アンサンブル学習  
  複数のモデルで学習すること
  - バギング  
    全体の一部のデータを複数のモデルで学習する方法  
    並行実行するのがミソ
  - ブースティング  
    一部のデータを繰り返し抽出し、複数のモデルで学習する  
    逐次実行するのがミソ
    - AdaBoost
    - 勾配ブースティング
    - XgBoost

#### サポートベクターマシン - Support Vector Machine -

マージン最大化のコンセプトで、２つのクラスを線形分離するアルゴリズム。  
**スラック変数**により、どの程度誤分類を許容するかを調整可能。

> マージン最大化  
> 入力データの各点との距離が最大になる境界線を求めるパターン分類

- 扱うデータが高次元の場合  
  超平面を考える
- 線形分離できない  
  **カーネル関数**（カーネル法）を使ってデータを高次元写像する。  
  計算式が複雑になるのを防ぐために**カーネルトリック**を用いる。

#### ニューラルネットワーク

飛ばしまーす

### 教師なし学習

- クラスタリング
  - k-means法

#### k-means

教師なし学習の中で、データの構造や特徴を掴むための手法の一つ。  
クラスタを作るやつ。

1. クラスタ数をk個と定めて、ランダムな位置に中心点を置く。  
   > だから**k**-meansなんだ。別にnでもいいじゃん。
2. クラスタの重心を移動させる
3. 各データを一番近い重心のクラスタに属させる。


#### 主成分分析(Principal Component Analysis)

これも飛ばす

## 手法の評価

- 交差検証  
  全データを学習用（訓練データ）と評価用（テストデータ）に分割して評価する
  - ホールドアウト検証  
    事前に訓練データとテストデータに分割する方法。  
    データが少ない場合、データに偏りが生じてたまたま評価が良くなるパターンが発生する。
  - k-分割交差検証  
    訓練データとテストデータの分割したあと、それらをローテーションして訓練データとテストデータのパターンを作る。そして、それらで学習と評価を行う。

データが分割されるパターン

- 全データ
  - 訓練データ
    - 訓練データ
    - 検証データ
  - テストデータ

## 評価指標

ベイズの定理で有名な問題を以下の**混同行列(Conduct Matrix)**で考える。

罹患率0.01%の病気について

|実際の値\予測値|陽性(Positive)|陰性(Negative)|
|:--:|:--:|:--:|
|罹患(Positive)|真陽性(True Positive)|偽陰性(False Negative)|
|非罹患(Negative)|偽陽性(False Positive)|真陰性(True Negative)|

- 正解率(Accuracy)  
  TP + TN / TP + TN + FP + FN  
  全体から正しく予測できた（当たった）確率。
- 適合率(Precision)  
  TP / TP + FP  
  陽性の中から、実際に罹患だった確率。
- 再現率(Recall)  
  TP / TP + FN  
  罹患の中から、実際に陽性だと判断できた確率
- F値(F measure)  
  (2 * precision * recall) / (precision + recall)  
  適合率と再現率の調和平均

### 正則化

- L1正則化（ラッソ回帰）  
  一部のパラメーラを０にすることで、特徴選択を行うことができる。
- L2正則化（リッジ回帰）  
  パラメータの大きさに応じて０に近づけることで、汎化された滑らかなモデルを得る。
- Elastic Net = L1 + L2

## 第５章 ディープラーニングの基本

### ディープラーニングの問題

隠れ層の数こそがディープラーニングの表現力であるものの、増やせば誤差が最後まで正しく反映されない問題を持つ。  
シグモイド関数の微分が小さく成りすぎて変化に値しなくなる。  
こういう問題を勾配消失問題と呼ぶ。

### 事前学習

2006年、ジェフリー・ヒントンが問題の解決策を提案。  
自己符号化器(autoencoder)の提案。

#### オートエンコーダ - autoencoder -

隠れ層＋可視層（入力層＋出力層のセット）の２層のネットワーク。  
出力を入力と同じにするような学習を行う。  
隠れ層は入力データの特徴を上手く圧縮表現したものと捉えたら良い。  
つまり、**次元削減**を行うことができる。

#### 積層オートエンコーダ(Stacked autoencoder)

オートエンコーダは出力層が入力層ということで、単純につなげただけ

- 入力層
- 隠れ層
- 出力層（入力層）
- 隠れ層
- 以下略

##### ファインチューニング

オートエンコーダは出力層が入力層ということでラベルの出力が不可能。  
よって、最後の層に回帰層を入れることで解決する。

> 回帰問題の場合は、線形回帰層  
> ２値分類問題の場合は、ロジスティック回帰層（シグモイド関数）  
> 多値分類問題の場合は、ロジスティック回帰層（ソフトマックス関数）  

ただし、回帰層にも重みの調整が必要なため、事前学習を終えて回帰層を足したらネットワーク全体で学習を行う必要がある。

よって、積層オートエンコーダは事前学習とファインチューニングで構成される。


#### 深層信念ネットワーク(deep belief network)

積層オートエンコーダを提唱したジェフリー・ヒントンが提唱したもう一つのネットワーク。  
こちらは**教師なし学習**で、**制限付きボルツマンマシン**を用いる。

## 第６章 ディープラーニングの手法

### 活性化関数

シグモイド関数の微分が勾配消失を呼び込むので、色々な関数が開発された。  
出力層の関数は、シグモイドもしくはソフトマックスでなければ回帰・分類できないが、ソレ以外の隠れ層での活性化関数は**任意の実数を非線形変換できれば何でも**良かった。

#### tanh関数

シグモイド関数よりマシぐらいの関数。

#### ReLU

ランプ関数の一種。

微分すると、

- x < 0の場合は0
- 0 < xの場合は1

tanh関数に比べて、0 < xとときは常に微分値１のため勾配消失しにくい。  
とは言えど、x < 0のときは微分値０のため勾配消失しないわけではない。

#### Leaky ReLU

0 < xのときに微妙な傾きをもたせた関数。  
微分値0になりにくいので、さらに勾配消失しにくい。

### 勾配降下法

「モデルの予測値と実測値との誤差をなくす」目的を達成する一つの方法。  
もうここらへんは無限に勉強したしええやろ

ただし、鞍点についてだけ。  
鞍点とは、多次元のグラフである一次元に関しては極小だが、ある次元に関しては極大という関数。


### テクニック

学習途中を制御するテクニックとして活性化関数の選択があったが、学習始めにもテクニックがある。

- ドロップアウト
- early stopping
- 正規化
- 標準化
- 白色化
- バッチ正規化

#### ドロップアウト

エポックごとにテキトーにネットワークを除外する方法。  
単純に各エポックで微妙に違うネットワークが出来上がるので、アンサンブル学習になる。

#### early stopping

誤差関数の値が右肩上がりになり始めてる段階で学習をやめること

> ジェフリー・ヒントン  
> "Beautiful FREE LUNCH"  
> ノーフリーランチ定理  
> 「あらゆる問題で性能の良い汎用最適化戦略は理論上不可能」

#### 正規化

データのスケールを統一する手法。  
基本的には、各パラメータを[0, 1]の範囲に収めるようにする。

特に画像データは[0, 255]なので、よく使われる。

> e.g. 家賃
> - 駅からの距離：x00メートル
> - 部屋数：n個
> - 犯罪率：0 - 100 %
> 
> スケールバラバラすぎる。犯罪率なんて0.00 - 1.0の間しか行き来しないのに、距離は無限大に膨らむことができてしまうのは問題。

#### 標準化

各特徴量を以下の条件になるように補正。

- 平均：0
- 分散：1

機械学習は、「入力のそれぞれの特徴量がどう動くと、出力がどう動くか」を見るので、各特徴量の分散を揃えることで**各特徴量の動きに対する感度を平等化**することができる。

#### 白色化

各特徴量を**無相関化した**上で標準化する。

#### バッチ正規化

バッチ正規化はある一定の層の結果を正規化すること。  
特に、活性化関数が動く直前で正規化する。

### CNN

- ネオコグニトロン  
  S細胞とC細胞を模したNN
- LeNet  
  畳み込み層とプーリング層を持つNN  
  プーリング層のことをサブサンプリング層と行ったりもするらしい。

#### 畳み込み

畳み込みフィルタを採用することで平行移動に強い**移動不変性**を持つ。  
> 回転不変性は持っていないらしい。マジ？そうだっけ？

計算量：**サイズ小畳込みxN < サイズ大畳込みx1**

WaveNetでは時系列データの分析・学習にも用いられる。

特徴マップサイズの算出

(入力画像サイズ + パディング * 2 - フィルタサイズ) / ストライドサイズ + 1

#### プーリング

特に学習する

- 最大プーリング
- 平均プーリング

#### 全結合

畳み込み層、プーリング層ともに出力は画像の形式でしかない。  
これをラベルの一次元データに変換する層が必要となる。

最近のCNNは**Global Average Pooling**を用いることで全結合層を用いないようになってきている。

### CNNの発展型

最近のCNNは1x1の畳み込みフィルターを採用して次元削減を行う。  
point wise convolutionと呼ばれる。

- AlexNet  
  ILSVRC2012の優勝モデル。  
  畳み込み→プーリングx2→畳み込み→畳み込み→畳み込み→プーリング→全結合x3
- VGG
- GoogLeNet  
  Inceptionモジュールを採用
- ResNet  
  Skip Connectionを採用

#### 転移学習

すでに学習済みのモデルに自分でちょっと手を加えただけで制度が出る時代。

### 再帰型ニューラルネットワーク - RNN -

時系列順にデータを学習する仕組みを採用したNN。  
過去の隠れ層が現在の隠れ層と連結している特徴を持つ。  

> 閉路（行って戻ってくる）を持つ。

BackPropagation Through-Timeと呼ばれる過去を遡る誤差逆伝播法を採用する。

#### LSTM(Long Short-Term Memory)

RNNは時系列データを扱う万能のネットワークではなかった。  
時間軸が深くなると、やはり勾配消失問題が発生した。  
過去をさかのぼり続けられない状態となる場面が多々あった。

原因は「今の時点では重みは小さいが、将来の時点では関係が密になって重みが大きい」という矛盾が発生したのだ。  
これを**入力重み衝突**と**出力重み衝突**と呼ぶ。。

LSTMブロックは以下のモジュールからなる。

- 入力ゲート  
  入力重み衝突を回避するゲート
- 忘却ゲート
  セルに誤差が過剰に停留することをリセットする
- 出力ゲート  
  出力重み衝突を回避するゲート
- セル(Constant Error Carousel)  
  誤差を内部に留めて、勾配消失を防ぐ

#### GRU(Gated Recurrent Unit)

LSTMは複雑な時系列を扱えるが、もちろんネットワーク自体も複雑。  
LSTMをより簡略化したネットワーク。

GRUブロック

- リセットゲート
- 更新ゲート

#### Bidirectional RNN

過去だけじゃなく、未来からも重みを予測しようぜってモデル。
映像のBフレームみたいな感じ。

#### RNN Encoder-Decoder

AutoEncoderチックに入力が時系列で出力も時系列が良いっていう場面で活躍するモデル。  
**sequence-to-sequence**と呼ばれる自然言語処理系で用いられる。

LSTMを２つ組み合わせたモデル。

#### Attention

これE資格出たー。

RNNの応用により時系列タスクでは高い精度を出せるようにはなった。  
一方で、RNNはLSTMにしろGRUにしろ過去のどの時点でがどれだけ影響を持っているかを求めていない。  
あくまで、それまでの時点の状態が隠れ層となっている。

そこで、**時間の重み**をネットワークに追加したのが**Attention**と呼ばれる機構。  
過去の時点それぞれの重みを学習することでc、時間の重みを考慮するモデルとなっている。  
直近の未来しか重み付けできないのではという懸念が上がっている。

### 深層強化学習

#### 強化学習

機械学習の３つ目の手法。

> 機械学習は主に次の３つ
> - 教師あり学習
> - 教師なし学習
> - 強化学習

**行動を学習する仕組み**といえる。  
ある環境下で、目的とするスコアを最大化するためにどう行動するかを学習する。

ロボティクス系でよく用いられる学習方法。  

#### なぜ教師あり学習がロボティクスでダメなのか？  

教師あり学習でロボティクスを学習するとなると、ロボットの関節の角度、動かす速度、歩幅などの入出力ペアが必要となる。  
強化学習では「上手に歩けたか？」という指標として**歩けた距離**のみを最大化するように学習していく。

これだけだと簡単に聞こえてしまうが、

- 状態をいかに表現できるか
- その状態に基づいて現実的な時間内で行動に結び付けられるか

という難しさをはらんでいる。

#### 深層強化学習

強化学習のディープラーニング版。  

- Deep Q-Network  
  特に強化学習でもQ学習をディープ化
- Double DQN
- Dueling Network
- Categorical DQN
- Rainbow

> AlphaGo  
> AlphaGoは状態と行動の評価にCNNを採用する。  
> また、探索に**モンテカルロ木探索**を採用する。

### 深層生成モデル

認識・識別タスクから**生成タスク**への変遷。

生成したいモノ

- データを元に、新しく別の何かを生成したい。（識別モデルの延長）  
  LSTMで次の文字を推測し続けることで文章を生成する
  > WaveNetが該当する。
- データそのものを生成したい。  
  元データの分布を推測し、似たようなデータを生成する（サンプリング）

#### WaveNet

音声は時系列データだが、RNNは用いない。
CNN（一次元）の畳み込みで音声の生成を行う。

> RNNはLSTMなど同様に過去データと連結したネットワークを持つモデル

magentaと違って音楽は認識できない。

#### 変分オートエンコーダ - Variational AutoEncoder -

通称、VAE。  
VAEは入力データの特徴を**統計分布に変換**する。  

> 統計分布は、**平均**と**分散**のこと。

入力データ→(encoder)→統計分布
ランダムデータ→(decoder)→データ生成（復元）

> autoencoderではサンプリングされたデータは役に立たず、深層学習ではほぼ用いられない。  

#### 敵対的生成ネットワーク - Generative Adversarial Network -

イアン・グッドフェローが提案した教師なし学習。  
通称、GAN。  
以下の２つの機能が登場する。

- Generator  
  ランダムベクトルを入力値とし、偽物データを作る
- Discriminator  
  画像を入力値とし、本物か偽物かを判定する

画像生成に派生している。

> 超解像とかそう。

ヤル・ルカン  
「機械学習において、この10年で最も面白いアイデア」

## 第７章 ディープラーニングの研究分野

### 画像認識分野

#### AlexNet


#### R-CNN(Regional CNN)

画像の分類と関心領域（Region of Interest）の切り出しを行う。  

- Faster RCNN
- YOLO(You Only Look Once)
- SSD(Single Shot Detector)

#### Semantic segmentation

セマンティック・セグメンテーション。  
画素単位で認識を行うことを目的とする。  
通常のROIよりも遥かに詳細な領域分割を行う。

完全畳込みネットワークで実装されるため、画像認識を行えない。

> 全層が畳込み層になっている。

そのため、入力画像の画素数*カテゴリー数分の出力層が必要になって大変。

CNNは畳み込んでしまうのでカーネル幅だけ近傍の入力を計算してしまう。  
特徴抽出の計算は下位に行くほど画像を荒くする問題を抱える。  
そのためアンサンプリング処理が必要となる。

セグネットも同様。

#### Instance segmentation

インスタンス・セグメンテーション。

### 自然言語処理分野

日本語の自然言語処理フロー

1. 形態素解析  
   単語などの最小単位に分割
2. データクレンジング  
   不要文字列の削除
3. ベクトル形式に変換
   BoWなどを用いる。
4. 重要度の評価  
   TF-IDFなどを用いる。

自然言語処理の解析技術

> 形態素...言葉で意味を持つ最小単位

|解析技術|説明|
|:--:|:--|
|形態素解析|テキストデータを文法と単語に基づいて形態素に分割し、品詞を判定する|
|構文解析|形態素解析に基づいて、形態素間の構文的関係を解析する|
|含意関係解析|２つの文の間に含意関係が成立するかを判別する|
|意味解析|構文解析に基づいて、意味を持つまとまりを判定する|
|文脈解析|文単位で構造や意味を考える|
|照応解析|照応詞の指示対象・省略された名詞の推定・補完|
|LDA|文中の単語からトピックを推定する教師なし学習|
|LSI|複数の文書に出てくる共通単語を解析し、低次元の洗剤意味空間を構成する|

- Latent Dirichlet Allocation(LDA)  
  ディリクレ分布を用い、各単語が「隠れた特定のトピック」から生成されているという仮定のもと推定する。  
  > ディリクレ分布  
  > 確率分布の一種。
- Latent Semantic Indexing(LSI)  
  特異値分解を使うらしい。  
  > 特異値分解  
  > ある行列を複数の行列の積で表現する方法。  
  > 次元削減などに用いられる。

#### word2vec

2013年トマス・ミコロフが提唱。
以下のように呼ばれる。

- ベクトル空間モデル  
  ベクトル間の距離や関係として単語の意味を表現する
- 単語埋め込みモデル  
  単語の意味をベクトル空間の中に表現した。  
  

「単語の意味は、その周辺の単語によって決まる」という主張を実現した

手法

- スキップグラム(skip-gram)  
  特定の単語 -> 周辺の単語を予測する
- CBOW  
  周辺の単語 -> 特定の単語を予測

#### fastText

トマス・ミコロフによって開発。
正式なword2vecの後継。  

word2vecと違って、単語の表現に文字の情報も含めたことが特徴。  
これにより、訓練データには存在しない単語を表現可能に。

> Out Of Vocabulary  
> 存在しない単語

学習時間が早いこともいい。

#### ELMo

アイレンスティチュートによって開発された文章表現を得るモデル。  
２層の双方向リカレントネットワーク言語モデルの内部状態から計算される。  

fasttextと同じく、文字ベースの単語表現を作るためOOVに対応可能。

#### マルチタスク言語モデル

fastTextやELMoによる単語埋め込みモデルの発展により可能に。

処理手順

1. 次文あるいは前文の予測
2. 機械翻訳
3. 構文解析
4. 自然言語推論

１対多のマルチタスク学習を用いる。  
これを普遍埋め込みモデルと表現する。

### 画像脚注付け

CNN + 言語モデル(リカレントニューラルネットワーク) = ニューラル画像脚注付け(Neural Image Captioning)

NICの入力にCNNの最終層を出力には使わない。  

### 音声認識

隠れマルコフモデルが使われていたが、最近ディープラーニングに置き換えられている、

> 隠れマルコフモデル  
> 「未来の状態が現状態にのみ依存する」というマルコフモデルの一つ。

#### WaveNet

以下の２つができる。
- 音声合成
- 音声認識

### 強化学習（ロボティクス）

Deep Q Networks

アルファ碁はCNNをモンテカルロ木探索する。

- 方策（ポリシー）ベース
  - UNREAL
- 行動価値関数ベース
  - Q関数
- モデルベース
  - A3C

これらすべてを持った最強の強化学習モデルをRAINBOWモデルという。

ロボットは入力信号に以下の統合データを用いる。
- 視覚情報
- 聴覚情報
- 運動系からのフィードバック情報

こういう複数の情報源を統合する学習をマルチモーダル学習と呼ぶ。


## 著名人のセリフ

- レイ・カーツワイル  
  「シンギュラリティは2045年に到来する」
- ヒューゴ・デ・カリス  
  「シンギュラリティは21世紀の後半に到来する」
- オレン・エツィオーニ  
  「たとえば100万年５、特異点を迎える可能性はある。けれど、賢いコンピュータが世界制覇するという終末論的構想は『馬鹿げている』としか言いようがない」
- ヴァーナー・ヴィンジ  
  「シンギュラリティとは、機械が人間の役に立つフリをしなくなること」
- スティーブン・ホーキング  
  「AIの完成は人類の終焉を意味するかもしれない」

## 統計学

データから何かしらの知見を得る手法。  

- 記述統計  
  手元のデータについて代表値を計算したり、傾向を分析する。
- 推計統計  
  データの背後に潜む母集団の性質を、手元のデータから明らかにする

> 選挙速報で開票数％の段階から当選確実と発表されるのは**崇敬統計**の応用例

## 勾配降下法

- 重み更新の回数：イテレーション
- 訓練データの学習回数：エポック
- 逐次学習：サンプル毎に学習する

### 学習方法

- バッチ学習  
  BatchSize = 全データ数  
  > 最急降下法
- ミニバッチ学習  
  1 < BatchSize < 全データ数
- オンライン学習（逐次学習とも）  
  BatchSize = 1  
  > 確率的勾配降下法

## コンペティション

### PASCAL Visual Object Classes Challenge

小規模データセット（1万件程度）の画像分類コンペティション。  
クラス数が20と少なく、現実的な問題としては不十分。

### ImageNet Large Scale Visual Recognition Challenge

通称、ILSVRC。  
画像認識・分類の技術的進歩を定量的に測るコンペティション。
2010年に以下のメンバーをOrganizerとしてスタート。

- Alex Berg
- Jia Deng
- Fei Fei Li

最初はImage classification annotationsのみのコンペティションだった。  
2017年では以下の３つのコンペティションとなっている。

- Object localization for 1000 categories  
  画像内に物体がどこにあるか
- Object detection for 200 fully labeled categories  
  画像内で指定された200カテゴリの物体検出
- Object detection from video for 30 fully labeled categories  
  動画内で指定されtあ30カテゴリの物体検出

2010年からスタートしたが、DeepLearningがブレイクするーしたのは2012年

|大会|1位モデル|2位モデル|
|:--:|:--:|:--:|
|201x|サポートベクターマシン||
|2012|AlexNet||
|2013|ZFNet||
|2014|GoogLeNet|VGGNet|
|2015|ResNet|


#### AlexNet

ILSVRC2012の優勝モデル。  

- Alex Krizhevsky
- Ilya Sutskever
- Geoffrey E. Hinton（ジェフリー・ヒントン）

５つの畳み込み層と３つの全結合層の全８層モデル。  
パラメータ数は6000万個。
局所解に陥らないようにReLU関数やDropoutやDataAugmentationを用いている。

#### ZFNet

ILSVRC2013の優勝モデル。

- Matthew D. Zeiler
- Rob Fergus

AlexNetと同じ層構造だが、以下で異なる。

- 畳み込み層
  - Kernel size: 11x11 → 7x7
  - Stride: 4 → 2
  - Kernel数: 384-384-256 → 512-1024-512  
    Deconvolution層を追加

#### VGGNet

ILSVRC2014の準優勝モデル。

- Karen Simonyan
- Andrew Zisserman

AlexNet(8層)よりも深い19層を成す。  
パラメータ数は１億4000万。

semantic segmentaionやSegNetに使われる。

#### GoogLeNet

ILSVRC2014の優勝モデル。

VGGNet(19)よりも深い22層。  
特徴として、Inceptionモジュールを採用している。  

https://ritsuan.com/blog/7673/

#### ResNet

ILSVRC2015の優勝モデル。

MicrosoftResearchAsia(MSRA)の４名が開発。

- Kaiming He
- Xiangyu Zhang
- Shaoqing Ren
- Jian Sun

152層。  
入力と出力を接続する経路を持つ誤差ユニットを持つことが特徴。  
Identity mappingと呼ばれる。

## 単語

- AI効果  
「単純な自動化であって知能とは関係ない」と結論づける人間心理  

- ロジック・セオリスト  
  世界初の人工知能プログラム

- 人工知能ってなんだ？  
  - 専門家にもわからない
  - 同じシステムでも専門家によって意見が違う

### ディープブルー

IBM製のチェスAI。  
ほぼ力任せの検索（ブルートフォース）だったが、それでも人間に打ち勝つ。  



MDL WAIC IB BIC CIC DIC EIC
ステップワイズ法


